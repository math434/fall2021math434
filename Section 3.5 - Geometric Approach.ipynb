{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "wrapped-orbit",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 3.5: Geometric Approach to the Least Squares Problem\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-turkish",
   "metadata": {},
   "source": [
    "Let $S$ be a nonempty subset of $\\mathbb{R}^n$.\n",
    "\n",
    "The _orthogonal complement_ of the set $S$ is\n",
    "\n",
    "$$\n",
    "S^\\perp = \\big\\{ x \\in \\mathbb{R}^n : \\langle x, y \\rangle = 0, \\forall y \\in S \\big\\}.\n",
    "$$\n",
    "\n",
    "This is the set of vectors $x$ that are orthogonal to every vector $y$ in the set $S$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-locator",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-money",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Prove that $S^\\perp$ is a subspace of $\\mathbb{R}^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-seventh",
   "metadata": {},
   "source": [
    "### Proof.\n",
    "\n",
    "First we show that $S^\\perp$ is closed under vector addition. Let $x, z \\in S^\\perp$, and we want to show that $x + z \\in S^\\perp$. Let $y \\in S$ chosen arbitrarily. Then we know that $\\langle x, y \\rangle = 0$ and $\\langle z, y \\rangle = 0$. Thus,\n",
    "\n",
    "$$\n",
    "\\langle x + z, y \\rangle = \\langle x, y \\rangle + \\langle z, y \\rangle = 0 + 0 = 0.\n",
    "$$\n",
    "\n",
    "Thus, $\\langle x + z, y \\rangle = 0$ for all $y \\in S$, so $x + z \\in S^\\perp$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-plumbing",
   "metadata": {},
   "source": [
    "Next we show that $S^\\perp$ is closed under scalar multiplication. Let $x \\in S^\\perp$ and $\\alpha \\in \\mathbb{R}$, and we want to show that $\\alpha x \\in S^\\perp$. Let $y \\in S$ be chosen arbitrarily. Then we know that $\\langle x, y \\rangle = 0$. Thus,\n",
    "\n",
    "$$\n",
    "\\langle \\alpha x, y \\rangle = \\alpha \\langle x, y \\rangle = \\alpha \\cdot 0 = 0.\n",
    "$$\n",
    "\n",
    "Thus, $\\langle \\alpha x, y \\rangle = 0$ for all $y \\in S$, so $\\alpha x \\in S^\\perp$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-coast",
   "metadata": {},
   "source": [
    "Since $S^\\perp$ is closed under vector addition and scalar multiplication, $S^\\perp$ is a subspace of $\\mathbb{R}^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-determination",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-structure",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Let $S$ be the $xy$-plane in $\\mathbb{R}^3$. Then\n",
    "\n",
    "$$\n",
    "S = \\left\\{ \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} \\in \\mathbb{R}^3 : z = 0 \\right\\}.\n",
    "$$\n",
    "\n",
    "The orthogonal complement of $S$ is\n",
    "\n",
    "$$\n",
    "S^\\perp = \\left\\{ \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} \\in \\mathbb{R}^3 : x = y = 0 \\right\\},\n",
    "$$\n",
    "\n",
    "which is the $z$-axis in $\\mathbb{R}^3$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-subscription",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-virus",
   "metadata": {},
   "source": [
    "> ### Theorem: (Subspace Decomposition of $\\mathbb{R}^n$)\n",
    ">\n",
    "> Let $S$ be a subspace of $\\mathbb{R}^n$. Then\n",
    ">\n",
    "> $$ \\mathbb{R}^n = S \\oplus S^\\perp. $$\n",
    ">\n",
    "> That is, every $x \\in \\mathbb{R}^n$ can be written **uniquely** as $x = y + z$, where $y \\in S$ and $z \\in S^\\perp$.\n",
    "> \n",
    "> The vector $y$ is the _orthogonal projection_ of $x$ into $S$, and $z$ is the _orthogonal projection_ of $x$ into $S^\\perp$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-turkish",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-american",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Let $S$ be the $xy$-plane in $\\mathbb{R}^3$ and let\n",
    "\n",
    "$$\n",
    "u = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Find the unique vectors $v \\in S$ and $w \\in S^\\perp$ such that $u = v + w$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-ethics",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Projecting $u$ orthogonally onto $S$ gives us\n",
    "\n",
    "$$\n",
    "v = \\begin{bmatrix} 1 \\\\ 2 \\\\ 0 \\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-wound",
   "metadata": {},
   "source": [
    "Then letting $w = u - v$, we have\n",
    "\n",
    "$$\n",
    "w = \\begin{bmatrix} 0 \\\\ 0 \\\\ 3 \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Note that $w \\in S^\\perp$, so we are done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-kingston",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-amsterdam",
   "metadata": {},
   "source": [
    "## Two Fundamental Subspaces\n",
    "\n",
    "Let $A \\in \\mathbb{R}^{m \\times n}$. The **null space** of $A$ is\n",
    "\n",
    "$$\n",
    "\\mathcal{N}(A) = \\big\\{ x \\in \\mathbb{R}^n : A x = 0 \\big\\}.\n",
    "$$\n",
    "\n",
    "The **range** (or **column space**) of $A$ is\n",
    "\n",
    "$$\n",
    "\\mathcal{R}(A) = \\big\\{ A x : x \\in \\mathbb{R}^n \\big\\}.\n",
    "$$\n",
    "\n",
    "Note that $\\mathcal{N}(A)$ is a subspace of $\\mathbb{R}^n$ and $\\mathcal{R}(A)$ is a subspace of $\\mathbb{R}^m$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-syracuse",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-chemical",
   "metadata": {},
   "source": [
    "> ### The Fundamental Theorem of Linear Algebra:\n",
    ">\n",
    "> $$\\mathcal{R}(A)^\\perp = \\mathcal{N}(A^T)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-heating",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-range",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Prove the Fundamental Theorem of Linear Algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressive-package",
   "metadata": {},
   "source": [
    "### Proof.\n",
    "\n",
    "First we want to show that $\\mathcal{R}(A)^\\perp \\subseteq \\mathcal{N}(A^T)$.\n",
    "\n",
    "Let $y \\in \\mathcal{R}(A)^\\perp$. Then $\\langle y, A x \\rangle = 0$, for all $x \\in \\mathbb{R}^n$. Since\n",
    "\n",
    "$$\n",
    "\\langle y, A x \\rangle = y^T A x = (A^T y)^T x = \\langle A^T y, x \\rangle\n",
    "$$\n",
    "\n",
    "we have that $\\langle A^T y, x \\rangle = 0$ for all $x \\in \\mathbb{R}^n$. Thus, $A^T y$ must be the zero vector, which implies that $y \\in \\mathcal{N}(A^T)$.\n",
    "\n",
    "Next we want to show that $\\mathcal{N}(A^T) \\subseteq \\mathcal{R}(A)^\\perp$.\n",
    "\n",
    "Let $y \\in \\mathcal{N}(A^T)$. Then $A^T y = 0$, so $\\langle A^T y, x \\rangle = 0$, for all $x \\in \\mathbb{R}^n$. Then, by the above observation, $\\langle y, A x \\rangle = 0$ for all $x \\in \\mathbb{R}^n$. Therefore, $y \\in \\mathcal{R}(A)^\\perp$.\n",
    "\n",
    "Therefore, $\\mathcal{R}(A)^\\perp = \\mathcal{N}(A^T)$. $\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "careful-livestock",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-february",
   "metadata": {},
   "source": [
    "## The Discrete Least Squares Problem\n",
    "\n",
    "The least squares problem of minimizing $\\|b - A x\\|_2$ can be written as the problem of finding the orthogonal projection of the vector $b$ into $\\mathcal{R}(A)$. That is,\n",
    "\n",
    "$$\n",
    "\\min_{x \\in \\mathbb{R}^n} \\|b - A x\\|_2 = \\min_{y \\in \\mathcal{R}(A)} \\|b - y\\|_2.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grand-drive",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-strand",
   "metadata": {},
   "source": [
    "> ### Theorem: (Normal Equations)\n",
    ">\n",
    "> Let $A \\in \\mathbb{R}^{m \\times n}$ and $b \\in \\mathbb{R}^m$. Then $x \\in \\mathbb{R}^n$ solves the least squares problem\n",
    ">\n",
    "> $$ \\min_{x \\in \\mathbb{R}^n} \\|b - A x\\|_2 $$\n",
    ">\n",
    "> if and only if $x$ satisfies the **normal equations**\n",
    ">\n",
    "> $$ A^T A x = A^T b. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-fabric",
   "metadata": {},
   "source": [
    "### Proof.\n",
    "\n",
    "Note that $\\mathbb{R}^m = \\mathcal{R}(A) \\oplus \\mathcal{R}(A)^\\perp = \\mathcal{R}(A) \\oplus \\mathcal{N}(A^T)$. Then, there exists unique vectors $y \\in \\mathcal{R}(A)$ and $r \\in \\mathcal{N}(A^T)$ such that \n",
    "\n",
    "$$b = y + r,$$\n",
    "\n",
    "where $y$ is the orthogonal projection of $b$ into $\\mathcal{R}(A)$ and $r$ is the orthogonal projection of $b$ into $\\mathcal{N}(A^T)$.\n",
    "\n",
    "First we assume that $x$ solves the least squares problem. Then $y = A x$ and $r = b - A x$. Since $r \\in \\mathcal{N}(A^T)$, we have that $A^T r = 0$, which implies that\n",
    "\n",
    "$$ A^T(b - A x) = 0. $$\n",
    "\n",
    "Therefore, $x$ satisfies the normal equations $A^T A x = A^T b$.\n",
    "\n",
    "Next we assume that $x$ satisfies the normal equations $A^T A x = A^T b$. Let $y = A x$ and $r = b - y$. Then $b = y + r$, with $y \\in \\mathcal{R}(A)$ and $r \\in \\mathcal{N}(A^T)$. Therefore, $y$ is the orthogonal projection of $b$ into $\\mathcal{R}(A)$, which implies that $x$ solves the least squares problem. $\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-zimbabwe",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-nickel",
   "metadata": {},
   "source": [
    "## Solving the Normal Equations\n",
    "\n",
    "Suppose that $A \\in \\mathbb{R}^{m \\times n}$ ($m > n$) has linearly independent columns. Then $A^T A$ is positive definite, so we can solve the normal equations\n",
    "\n",
    "$$ A^T A x = A^T b $$\n",
    "\n",
    "using **Cholesky's method**.\n",
    "\n",
    "If $A$ is well-conditioned (i.e., the condition number $\\kappa_2(A)$ is small), then this approach is safe.\n",
    "\n",
    "However, if $\\kappa_2(A)$ is not small, then\n",
    "\n",
    "$$ \\kappa_2(A^T A) = \\kappa_2(A)^2 $$\n",
    "\n",
    "implies that $A^T A$ will be ill-conditioned, so solving $A^T A x = A^T b$ is not a good idea since it will likely be less accurate than the $QR$ approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-actor",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defensive-fundamental",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-revision",
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "# Create a Vandermonde matrix\n",
    "vandermonde(n,m) = [(j/m)^(i-1) for i=1:n, j=1:m]\n",
    "\n",
    "n, m = 15, 10\n",
    "A = vandermonde(n, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-recovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-heater",
   "metadata": {},
   "outputs": [],
   "source": [
    "AtA = A'A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-oakland",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond(AtA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-projection",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond(A)^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-rogers",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = randn(n);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-heather",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the QR approach to solve the least squares problem\n",
    "\n",
    "F = qr(A)\n",
    "\n",
    "c = F.Q'b\n",
    "\n",
    "xqr = F.R\\c[1:m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-excuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm(b - A*xqr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-engineer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Cholesky's method to solve the normal equations\n",
    "\n",
    "F = cholesky(AtA)\n",
    "\n",
    "xne = F\\(A'b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-edinburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm(b - A*xne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-engineering",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm(b - A*xqr) < norm(b - A*xne)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-bennett",
   "metadata": {},
   "source": [
    "We see that the $QR$ approach produces a solution with a smaller residual than the normal equation approach. Thus, it appears that the $QR$ approach is more accurate for solving this least squares problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-season",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.3",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
